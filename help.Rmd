---
title: "help"
author: "Miguel Figueira"
date: "9 de abril de 2016"
output: html_document
---


# Help.csv


```{r setup,warning=FALSE}
library("knitr")
library("rgl")
library("rglwidget")
knit_hooks$set(webgl = hook_webgl)

```


```{r, echo=F,warning=FALSE}

codo.jambu= function(data){
  aux.jambu <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:15) aux.jambu[i] <- sum(kmeans(data, centers=i)$withinss)
  plot(1:15,
       aux.jambu,
       type="b",
       xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}



ds.clust3d = function(ds,distan,metodo,nclass,clase = NULL){
  
  print(paste("el metodo es:",metodo,"usando distancia:",distan))
  # Encontramos modelo
  ds.matrix <- as.matrix(ds)
  
  distancia <- dist(x = ds.matrix , method = distan)
  
  cluster <- hclust(d = distancia,method = metodo)
  
  corte = cutree(tree = cluster, k= nclass)
  
  plot3d(x = ds$V1,
         y = ds$V2,
         z = ds$V3,
         col = corte,
         main = paste("Plot del corte con",nclass,"clases"))
  
  if(!is.null(clase)){
    matriz.confusion<- table(clase,corte)
    print("matriz de confusión")
    print(matriz.confusion)
    print(paste("precision:",accuracy <- sum(diag(matriz.confusion))/sum(matriz.confusion)))
    
  }
  V1 <- c(paste(metodo))
  V2 <- c(paste(distan))
  V3 <- c(paste(sum(diag(matriz.confusion))/sum(matriz.confusion)))
  b <- as.data.frame(V1)
  b$V2 <- V2
  b$V3 <- V3
  return(b)
  
}

```


1.Leer el archivo 
```{r, echo=T,warning=FALSE}
setwd("C:/Users/Alex/Documents/R/DM_T3/AprendizajeNoSupervisado")
help <- read.csv(file = "datos/help.csv",header = F)
```




2.Luego vemos el codo de jambu para ver cuál es el mejor K
```{r, echo=F,warning=FALSE}
codo.jambu(help[ ,c(1,2,3)])
```
De esta gráfica se determina que lo ideal son 3 o 4 clusters. Yo decidí usar 3.


3.Plot del dataset sin clusters ( sin colores)

```{r testgl, webgl=TRUE,warning=FALSE} 
plot3d(x = help$V1,
       y = help$V2,
       z = help$V3,
       main = "Plot del dataset s.csv")
```

# Pregunta 1
¿Cuántos clústers ve en el dataset help ?

Observando el dataset nos hace creer que son 3 clusters , es decir cada "letra" sería un cluster

 
4. Determino la regla de asignación de clases de manera equitativa
```{r, echo=T,warning=FALSE}
aux<- help$V4[order(help$V4)]

definir_clase = function(numero){
  if(numero <= max(aux[1:(0.333*length(aux))]))
    return(1)
  else if(numero <= max(aux[1:(0.666*length(aux))]))
    return(2)
  else
    return(3)
}

colo <- sapply(help$V4,definir_clase)
```


5. Gráfica del dataset con la regla de asignación de clases:

```{r testgl2, webgl=TRUE,warning=FALSE} 
plot3d(x = help$V1,
       y = help$V2,
       z = help$V3,
       col = colo,
       main = "Plot del dataset help.csv")

```


# Pregunta 2
¿Qué pasa al aplicar la regla de asignación de clases en este dataset?

Al ver la gráfica aplicando la regla de asignación de clases nos damos cuenta de que está totalmente mal como se asignan las clases , ya que tenemos los cluster negro y rojo dividos y de por medio otro cluster.


# Pregunta 3
¿Qué solución daría para asignar de manera correcta los valores de las clases y pueda analizar el desempeño del algoritmo de clustering de manera correcta?


Observando la gráfica, se puede usar el valor de la "X" que en este caso es help$V1 para asignar de mejor manera los valores

```{r, echo=T,warning=FALSE}
aux<- help$V4[order(help$V4)]

definir_clase = function(numero){
  if(numero <= 17)
    return(1)
  else if(numero <= 40.5)
    return(2)
  else
    return(3)
}

help$V4 <- sapply(help$V1,definir_clase)
```



6. Gráfica del dataset con la regla correcta  de asignación de clases:

```{r testgl3, webgl=TRUE,warning=FALSE} 
plot3d(x = help$V1,
       y = help$V2,
       z = help$V3,
       col = help$V4,
       main = "Plot del dataset help.csv")
```



## Aplicando k-media

Se usa el ciclo ya que el algoritmo k-media asigna cualquier centro a cada cluster y los crea de distinto "color" o distinto valor , por lo tanto para poder realizar una matriz de confusión que de verdad sirva para comparar es necesario tomar aquel que dé mayor precisión

```{r, echo=T,warning=FALSE}
accuracy <- 0

for(i in 1:100){
  aux.kmedias = kmeans(x = help[ ,c(1,2,3)],
                      centers = 3)
  matriz <- table(aux.kmedias$cluster, help$V4)
  
  aux.accuracy <- sum(diag(matriz))/sum(matriz)
  if(aux.accuracy > accuracy){
    accuracy <- aux.accuracy
    h.kmedias <- aux.kmedias
  }
}
```


## Matriz de confusión:
```{r, echo=T,warning=FALSE}
print(matriz.confusion <- table(h.kmedias$cluster,help$V4))
```


## Precisión
```{r, echo=T,warning=FALSE}
accuracy <- sum(diag(matriz.confusion))/sum(matriz.confusion)
print(accuracy)
```

### Gráfica del k-media

```{r testgl4, webgl=TRUE,warning=FALSE} 
plot3d(x = help$V1,
       y = help$V2,
       z = help$V3,
       col = h.kmedias$cluster,
       main = "Plot del k-media del dataset h.csv")
```


```{r, echo=F,warning=FALSE}
# crea el dataframe de la precisión de k-media
# irrelevante para el informe

V1 <- c(paste("k-media"))
V2 <- c(paste("NA"))
V3 <- c(paste(accuracy))
df.kmedia <- as.data.frame(V1)
df.kmedia$V2 <- V2
df.kmedia$V3 <- V3
```



## Cluster j


### Usando el método "ward.D"

```{r testgl11, webgl=TRUE} 
df <- ds.clust3d(ds = help[,c(1,2,3)],distan = "euclidean", metodo = "ward.D",
           nclass = 3, clase = help$V4)

```




### Usando el método "ward.D2"

```{r testgl14, webgl=TRUE} 
df4 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "euclidean", metodo = "ward.D2",
           nclass = 3, clase = help$V4)
df <-rbind(df,df4)
```

```{r testgl15, webgl=TRUE} 
df5 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "maximum", metodo = "ward.D2",
           nclass = 3, clase = help$V4)
df <-rbind(df,df5)
```



### Usando el método "complete"

```{r testgl20, webgl=TRUE} 
df10 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "euclidean", metodo = "complete",
           nclass = 3, clase = help$V4)
df <-rbind(df,df10)
```




### Usando el método "average"

```{r testgl23, webgl=TRUE} 
df13 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "euclidean", metodo = "average",
           nclass = 3, clase = help$V4)
df <-rbind(df,df13)
```



### Usando el método "mcquitty"

```{r testgl28, webgl=TRUE} 
df18 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "manhattan", metodo = "mcquitty",
           nclass = 3, clase = help$V4)
df <-rbind(df,df18)
```   



### Usando el método "median"

```{r testgl29, webgl=TRUE} 
df19 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "euclidean", metodo = "median",
           nclass = 3, clase = help$V4)
df <-rbind(df,df19)
```

```{r testgl30, webgl=TRUE} 
df20 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "maximum", metodo = "median",
           nclass = 3, clase = help$V4)
df <-rbind(df,df20)
```

```{r testgl31, webgl=TRUE} 
df21 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "manhattan", metodo = "median",
           nclass = 3, clase = help$V4)
df <-rbind(df,df21)
```  


### Usando el método "centroid"


```{r testgl33, webgl=TRUE} 
df23 <- ds.clust3d(ds = help[,c(1,2,3)],distan = "maximum", metodo = "centroid",
           nclass = 3, clase = help$V4)
df <-rbind(df,df23)
```



## Tabla con todos los accuracy
```{r, echo=T,warning=FALSE}
df <- rbind(df,df.kmedia)
colnames(df) <- c("metodo","distancia","accuracy")
df<- df[order(df$accuracy,decreasing = TRUE), ] 
print(df)
```


## Conclusiones 

- otros que dan igual que ward.D con distancia euclidean:(por lo tanto no se muestran), además todos dan 1 de precisión.

1. ward.D con distancia manhattan
2. ward.D con distancia maximum
3. centroid con distancia manhattan
4. centroid con distancia euclidean
5. mcquitty con distancia maximum
6. mcquitty con distancia euclidean
7. average con distancia manhattan
8. average con distancia maximum
9. complete con distancia manhattan
10. complete con distancia maximum
11. single con cualquier distancia
12. ward.D2 con distancia manhattan


- Como algoritmo escogería cualquiera de estos , ya que da precisión 1.







