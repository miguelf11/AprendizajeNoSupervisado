---
title: "guess"
author: "Miguel Figueira"
date: "9 de abril de 2016"
output: html_document
---

# Guess.csv


```{r, echo=F,warning=FALSE}



ds.clust = function(ds,distan,metodo,nclass){
  
  print(paste("el metodo es:",metodo,"usando distancia:",distan))
  # Encontramos modelo
  ds.matrix <- as.matrix(ds)
  
  distancia <- dist(x = ds.matrix , method = distan)
  
  cluster <- hclust(d = distancia,method = metodo)
  
  
  corte = cutree(tree = cluster, k= nclass)
  plot(x = ds$V1,
       y = ds$V2,
       col = corte,
       xlab = "X",
       ylab = "Y",
       main = paste("Plot del corte con",nclass,"clases"))
  
  
}

distancias <- c("euclidean","maximum","manhattan")


hclust.metodos <- c("ward.D","ward.D2", "complete", "average", "mcquitty","median")
```



### 1.Leer el archivo 
```{r, echo=T,warning=FALSE}
guess <- read.csv(file = "datos/guess.csv",header = F)
```


### Implementación del código de Jambu

```{r, echo=T,warning=FALSE}
codo.jambu= function(data){
  aux.jambu <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:15){
    aux.jambu[i] <- sum(kmeans(data, centers=i)$withinss)
  } 
  plot(x = 1:15,
       y = aux.jambu,
       type="b",
       xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}
```



### 2. Codo de Jambu
```{r, echo=F,warning=FALSE}
codo.jambu(guess)
```
El eje X es la cantidad de cluster y el eje Y  es la suma de los cuadrados de la distancia de las instancias dentro del cluster , la idea es que sea lo menor posible , pero tampoco tiene sentido hacer n clusters, donden n sea la cantidad de instancias,  por lo tanto usando esta gráfica se puede observar cuando es bueno crear más clusters dependiendo de la pendiente entre una cantidad de cluster y la siguiente

Por ejemplo en este caso se podría usar 3 o 4 ya que la "pendiente" o la "diferencia" entre 4 y 5 clusters es prácticamente mínima.

En mi caso usaré 3 ya que considero que no existe tanta diferencia entre tener 3 y 4 clusters en lo que se refieren a la suma cuadrada de los "withinss"



### 3. Gráfica del dataset
```{r, echo=F,warning=FALSE}
plot(x = guess$V1,
     y = guess$V2,
     xlim = c(min(guess$V1-0.5), max(guess$V1+0.5)),
     ylim = c(min(guess$V2-0.5), max(guess$V2+0.5)),
     xlab = "X",
     ylab = "Y",
     main = "Clustering Rectangular del dataset guess")
```

- Evaluando ambos gráficos , lo mejor es formar 3 clusters


## k -media

### Calculando k-media
```{r,echo=T}
guess.kmedias = kmeans(x = guess,
                   centers = 3)
```


### Gráfico del k-media del dataset 
```{r,echo=F}
plot(x = guess$V1,
     y = guess$V2,
     col = guess.kmedias$cluster,
     main = "k-media dataset guess.csv")

points(x = guess.kmedias$centers[, c("V1", "V2")],
       col = 1:4, pch = 19, cex = 3)
```


## hclust

```{r,echo=F}
for(i in hclust.metodos){
  for(j in distancias){
    ds.clust(ds = guess,metodo = i,distan = j,nclass = 3)
  }
}
```



## Conclusión:

### Metodos y distancias que funcionan mal para este dataset:
- Metodo centroid solo crea 2 clusters por lo tanto tampoco se incluirá las gráficas en el informe.
-Metodo single funciona bastante mal ,los agrupa todos como un mismo cluster(por lo tanto no se incluyen en el informe)
- Metodo average con distancia distinta a la euclidiana
- Método average con cualquier distancia tampoco funciona muy bien en este dataset.


Los demás métodos y distancias podrían servir , realmente no sabemos ya que no sabemos cuál debería ser el resutado.



 